# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['neofuzz']

package_data = \
{'': ['*']}

install_requires = \
['pynndescent>=0.5.0,<0.6.0', 'scikit-learn>=1.1.0,<1.3.0']

setup_kwargs = {
    'name': 'neofuzz',
    'version': '0.1.3',
    'description': 'Blazing fast fuzzy text search for Python.',
    'long_description': '# Neofuzz\n\nBlazing fast, lightweight and customizable fuzzy and semantic text search in Python.\n\n## Introduction ([Documentation](https://x-tabdeveloping.github.io/neofuzz/))\nNeofuzz is a fuzzy search library based on vectorization and approximate nearest neighbour\nsearch techniques.\n\n### Why is Neofuzz fast?\nMost fuzzy search libraries rely on optimizing the hell out of the same couple of fuzzy search algorithms (Hamming distance, Levenshtein distance). Sometimes unfortunately due to the complexity of these algorithms, no amount of optimization will get you the speed, that you want.\n\nNeofuzz makes the realization, that you can’t go above a certain speed limit by relying on traditional algorithms, and uses text vectorization and approximate nearest neighbour search in the vector space to speed up this process.\n\nWhen it comes to the dilemma of speed versus accuracy, Neofuzz goes full-on speed.\n\n### When should I choose Neofuzz?\n - You need to do repeated searches in the same corpus.\n - Levenshtein and Hamming distance is simply not fast enough.\n - You are willing to sacrifice the quality of the results for speed.\n - You don’t mind that the up-front computation to index a corpus might take time.\n - You have very long strings, where other methods would be impractical.\n - You want to rely on semantic content.\n - You need a drop-in replacement for TheFuzz.\n\n### When should I NOT choose Neofuzz?\n - The corpus changes all the time, or you only want to do one search in a corpus. (It might still give speed-up in that case though.)\n - You value the quality of the results over speed.\n - You don’t mind slower searches in favor of no indexing.\n - You have a small corpus with short strings.\n\n## [Usage](https://x-tabdeveloping.github.io/neofuzz/getting_started.html)\n\nYou can install Neofuzz from PyPI:\n\n```bash\npip install neofuzz\n```\n\nIf you want a plug-and play experience you can create a generally good quick and dirty\nprocess with the `char_ngram_process()` process.\n\n```python\nfrom neofuzz import char_ngram_process\n\n# We create a process that takes character 1 to 5-grams as features for\n# vectorization and uses a tf-idf weighting scheme.\n# We will use cosine distance for the nearest neighbour search.\nprocess = char_ngram_process(ngram_range=(1,5), metrics="cosine", tf_idf=True)\n\n# We index the options that we are going to search in\nprocess.index(options)\n\n# Then we can extract the ten most similar items the same way as in\n# thefuzz\nprocess.extract("fuzz", limit=10)\n---------------------------------\n[(\'fuzzer\', 67),\n (\'Januzzi\', 30),\n (\'Figliuzzi\', 25),\n (\'Fun\', 20),\n (\'Erika_Petruzzi\', 20),\n (\'zu\', 20),\n (\'Zo\', 18),\n (\'blog_BuzzMachine\', 18),\n (\'LW_Todd_Bertuzzi\', 18),\n (\'OFU\', 17)]\n```\n\n## [Custom Processes](https://x-tabdeveloping.github.io/neofuzz/custom_vectorizer.html)\n\nYou can customize Neofuzz’s behaviour by making a custom process.\nUnder the hood every Neofuzz Process relies on the same two components:\n\n - A vectorizer, which turns texts into a vectorized form, and can be fully customized.\n - Approximate Nearest Neighbour search, which indexes the vector space and can find neighbours of a given vector very quickly. This component is fixed to be PyNNDescent, but all of its parameters are exposed in the API, so its behaviour can also be altered at will.\n\n### Words as Features\n\nIf you’re more interested in the words/semantic content of the text you can also use them as features. This can be very useful especially with longer texts, such as literary works.\n\n```python\nfrom neofuzz import Process\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n # Vectorization with words is the default in sklearn.\n vectorizer = TfidfVectorizer()\n\n # We use cosine distance because it\'s waay better for high-dimentional spaces.\n process = Process(vectorizer, metric="cosine")\n```\n\n### Dimentionality Reduction\n\nYou might find that the speed of your fuzzy search process is not sufficient. In this case it might be desirable to reduce the dimentionality of the produced vectors with some matrix decomposition method or topic model.\n\nHere for example I use NMF (excellent topic model and incredibly fast one too) too speed up my fuzzy search pipeline.\n\n```python\nfrom neofuzz import Process\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import NMF\nfrom sklear.pipeline import make_pipeline\n\n# Vectorization with tokens again\nvectorizer = TfidfVectorizer()\n# Dimentionality reduction method to 20 dimentions\nnmf = NMF(n_components=20)\n# Create a pipeline of the two\npipeline = make_pipeline(vectorizer, nmf)\n\nprocess = Process(pipeline, metric="cosine")\n```\n\n### Semantic Search/Large Language Models\n\nWith Neofuzz you can easily use semantic embeddings to your advantage, and can use both attention-based language models (Bert), just simple neural word or document embeddings (Word2Vec, Doc2Vec, FastText, etc.) or even OpenAI’s LLMs.\n\nWe recommend you try embetter, which has a lot of built-in sklearn compatible vectorizers.\n```bash\npip install embetter\n```\n\n```python\nfrom embetter.text import SentenceEncoder\nfrom neofuzz import Process\n\n# Here we will use a pretrained Bert sentence encoder as vectorizer\nvectorizer = SentenceEncoder("all-distilroberta-v1")\n# Then we make a process with the language model\nprocess = Process(vectorizer, metric="cosine")\n\n# Remember that the options STILL have to be indexed even though you have a pretrained vectorizer\nprocess.index(options)\n```\n',
    'author': 'Márton Kardos',
    'author_email': 'power.up1163@gmail.com',
    'maintainer': 'None',
    'maintainer_email': 'None',
    'url': 'None',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'python_requires': '>=3.8',
}


setup(**setup_kwargs)
