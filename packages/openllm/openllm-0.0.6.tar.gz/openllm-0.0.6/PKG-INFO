Metadata-Version: 2.1
Name: openllm
Version: 0.0.6
Summary: OpenLLM: REST/gRPC API server for running any open Large-Language Model - StableLM, Llama, Alpaca, Dolly, Flan-T5, Custom
Project-URL: Documentation, https://github.com/llmsys/openllm#readme
Project-URL: Issues, https://github.com/llmsys/openllm/issues
Project-URL: Source, https://github.com/llmsys/openllm
Author-email: Aaron Pham <aarnphm@bentoml.com>, BentoML Team <contact@bentoml.com>
License-Expression: Apache-2.0
License-File: LICENSE
Keywords: AI,Alpaca,BentoML,Generative AI,LLMOps,Large Language Model,MLOps,Model Deployment,Model Serving,PyTorch,Stable Diffusion,StableLM,Transformers
Classifier: Development Status :: 4 - Beta
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3 :: Only
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: Implementation :: CPython
Classifier: Programming Language :: Python :: Implementation :: PyPy
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries
Requires-Python: >=3.8
Requires-Dist: bentoml
Requires-Dist: black[jupyter]==23.3.0
Requires-Dist: filetype
Requires-Dist: grpcio
Requires-Dist: grpcio-health-checking
Requires-Dist: grpcio-reflection
Requires-Dist: inflection
Requires-Dist: opentelemetry-instrumentation-grpc==0.35b0
Requires-Dist: orjson
Requires-Dist: pillow
Requires-Dist: protobuf
Requires-Dist: pydantic
Requires-Dist: transformers[accelerate,onnx,onnxruntime,tokenizers,torch]>=4.29.0
Description-Content-Type: text/markdown

<div align="center">
    <h1 align="center">OpenLLM</h1>
    <br>
    <strong>REST/gRPC API server for running any Open Large-Language Model - StableLM, Llama, Alpaca, Dolly, Flan-T5, and more<br></strong>
    <i>Powered by BentoML ğŸ± + HuggingFace ğŸ¤—</i>
    <br>
</div>

To get started, simply install OpenLLM with pip:

```bash
pip install openllm
```

> NOTE: Currently, OpenLLM is built with pydantic v2. At the time of writing,
> Pydantic v2 is still in alpha stage. To get pydantic v2, do
> `pip install -U --pre pydantic`

To start a LLM server, `openllm start` allows you to start any supported LLM
with a single command. For example, to start a `dolly-v2` server:

```bash
openllm start dolly-v2

# Starting LLM Server for 'dolly_v2'
#
# 2023-05-27T04:55:36-0700 [INFO] [cli] Environ for worker 0: set CPU thread coun t to 10
# 2023-05-27T04:55:36-0700 [INFO] [cli] Prometheus metrics for HTTP BentoServer f rom "_service.py:svc" can be accessed at http://localhost:3000/metrics.
# 2023-05-27T04:55:36-0700 [INFO] [cli] Starting production HTTP BentoServer from "_service.py:svc" listening on http://0.0.0.0:3000 (Press CTRL+C to quit)
```

To see a list of supported LLMs, run `openllm start --help`.

On a different terminal window, open a IPython session and create a client to
start interacting with the model:

```python
>>> import openllm
>>> client = openllm.client.HTTPClient('http://localhost:3000')
>>> client.query('Explain to me the difference between "further" and "farther"')
```

To package the LLM into a Bento, simply use `openllm build`:

```bash
openllm build dolly-v2
```

ğŸ¯ To streamline production deployment, you can use the following:

- [â˜ï¸ BentoML Cloud](https://l.bentoml.com/bento-cloud): the fastest way to
  deploy your bento, simple and at scale
- [ğŸ¦„ï¸ Yatai](https://github.com/bentoml/yatai): Model Deployment at scale on
  Kubernetes
- [ğŸš€ bentoctl](https://github.com/bentoml/bentoctl): Fast model deployment on
  AWS SageMaker, Lambda, ECE, GCP, Azure, Heroku, and more!
