{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### üö© *Create a free WhyLabs account to complete this example!*<br> \n",
    ">*Did you know you can store, visualize, and monitor whylogs profiles with the [WhyLabs Observability Platform](https://whylabs.ai/whylabs-free-sign-up?utm_source=github&utm_medium=referral&utm_campaign=langkit)? Sign up for a [free WhyLabs account](https://whylabs.ai/whylogs-free-signup?utm_source=github&utm_medium=referral&utm_campaign=LLM_to_WhyLabs) to leverage the power of whylogs and WhyLabs together!*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring GPT with LangKit in WhyLabs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not using OpenAI? LangKit supports all models, both public API and self-hosted. We would love to hop on a quick call to show you how to monitor your LLM. [Grab time here with LangKit engineers](https://calendly.com/whylabs/langkit-feedback).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/whylabs/langkit/blob/beta%2Fexamples/langkit/examples/GPT_Intro_to_LangKit.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll show how to send your LLM metrics to your monitoring dashboard at WhyLabs Platform.\n",
    "We will:\n",
    "\n",
    "- Install LangKit\n",
    "- Define environment variables with the appropriate Credentials and IDs\n",
    "- Connect LangKit to WhyLabs\n",
    "- Generate telemetry on your prompts and responses and send to WhyLabs\n",
    "- Explore LangKit telemetry in WhyLabs\n",
    "- Monitor your LLM Application"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing LangKit\n",
    "\n",
    "First, let's install a beta build of __langkit__. Note: you may need to restart the kernel to use updated packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langkit==0.0.1b1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the installation worked run the following \"Hello, World!\" example (note the first time you import these langkit modules it will need to download models):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whylogs as why\n",
    "from whylogs.experimental.core.udf_schema import udf_schema\n",
    "from langkit.input_output import *\n",
    "from langkit.sentiment import *\n",
    "from langkit.regexes import *\n",
    "from langkit.textstat import *\n",
    "from langkit.themes import *\n",
    "from langkit.toxicity import *\n",
    "\n",
    "langkit_schema= udf_schema()\n",
    "profile_view = why.log({\"prompt\": \"Hello,\", \"response\": \"World!\"}, schema=langkit_schema).view()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This verifies that the installation and initialization of models worked, and now this text_schema can be used to profile you LLM. If you are curious what kinds of metrics are gathered you can see a dictionary of the metrics gathered on this Hello, World! example by running something like this:\n",
    "```python\n",
    "    print(profile_view.get_column(\"prompt\").get_metric(\"udf\").to_summary_dict())\n",
    "    print(profile_view.get_column(\"similarity_all-MiniLM-L6-v2\").to_summary_dict())\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úîÔ∏è Setting the Environment Variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to send our profile to WhyLabs, let's first set up an account. You can skip this if you already have an account and a model set up.\n",
    "\n",
    "We will need three pieces of information:\n",
    "\n",
    "- API tokens for the LLM and WhyLabs\n",
    "- Organization ID for WhyLabs\n",
    "- Dataset ID for WhyLabs\n",
    "\n",
    "\n",
    "**Go to [https://whylabs.ai/free](https://whylabs.ai/whylabs-free-sign-up?utm_source=github&utm_medium=referral&utm_campaign=langkit)** and grab a free account. You can follow along with the examples if you wish, but if you‚Äôre interested in only following this demonstration, you can go ahead and skip the quick start instructions.\n",
    "\n",
    "After that, you‚Äôll be prompted to create an **API token**. Once you create it, copy and store it locally. The second important information here is your org ID. Take note of it as well. After you get your API Token and **Org ID**, you can go to https://hub.whylabsapp.com/models to see your projects dashboard. You can create a new project and take note of it's **Dataset ID** (if it's a model project it will look like `model-xxxx`).\n",
    "\n",
    "*Note: If using OpenAI chat completion, We recommend testing this integration with a paid account using gpt-3.5-turbo: https://platform.openai.com/docs/guides/chat*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now set the credentials as environment variables. The WhyLabs Writer will check for the existence of these variables in order to send the profiles to your dashboard. In a production setting these would already be set as environment variables, but "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "whylabs_api_key = os.environ.get(\"WHYLABS_API_KEY\")\n",
    "org_id = os.environ.get(\"WHYLABS_DEFAULT_ORG_ID\")\n",
    "dataset_id = os.environ.get(\"WHYLABS_DEFAULT_DATASET_ID\")\n",
    "if whylabs_api_key is None:\n",
    "    print(\"Enter your WhyLabs API key\")\n",
    "    os.environ[\"WHYLABS_API_KEY\"] = getpass.getpass()\n",
    "    print(\"Using API Key ID: \", os.environ[\"WHYLABS_API_KEY\"][0:10])\n",
    "else:\n",
    "    print(\"Whylabs API Key already set with ID: \", os.environ[\"WHYLABS_API_KEY\"][0:10])\n",
    "if org_id is None:\n",
    "    print(\"Enter your WhyLabs Org ID\")\n",
    "    os.environ[\"WHYLABS_DEFAULT_ORG_ID\"] = input()\n",
    "    org_id = os.environ.get(\"WHYLABS_DEFAULT_ORG_ID\")\n",
    "else:\n",
    "    print(f\"WhyLabs Org ID is already set in env var to: {org_id}\")\n",
    "if dataset_id is None:\n",
    "    # set your datased_id (or model_id) here - should be something like \"model-xxxx\"\n",
    "    print(\"Enter your WhyLabs Dataset ID\")\n",
    "    os.environ[\"WHYLABS_DEFAULT_DATASET_ID\"] = input()\n",
    "    dataset_id = os.environ.get(\"WHYLABS_DEFAULT_DATASET_ID\")\n",
    "else:\n",
    "    print(f\"WhyLabs Dataset ID is already set in env var to: {dataset_id}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Basic OpenAI gpt-3.5-turbo integration:\n",
    "\n",
    "```python\n",
    "import os\n",
    "from typing import Optional\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "class ChatLog:\n",
    "    def __init__(self, prompt: str, response: str, errors: Optional[str] = None):\n",
    "        self.prompt = prompt\n",
    "        self.response = response\n",
    "        self.errors = errors\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"prompt\": self.prompt,\n",
    "            \"response\": self.response,\n",
    "            \"errors\" : self.errors\n",
    "        }\n",
    "\n",
    "# this is just for demonstration purposes\n",
    "def send_prompt(prompt: str) -> ChatLog:\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"The following is a conversation with an AI assistant.\"\n",
    "            }, {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }],\n",
    "            temperature=0.9,\n",
    "            max_tokens=100,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0.6\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return ChatLog(prompt, \"\", f\"{e}\")\n",
    "\n",
    "    result = ''\n",
    "    for choice in response.choices:\n",
    "        result += choice.message.content\n",
    "\n",
    "    return ChatLog(prompt, result)\n",
    "```\n",
    "\n",
    "If you have an integration like the above or another LLM, you can log the `ChatLog`'s dictionary using a LangKit schema and a `whylogs` rolling logger."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úçÔ∏è Connect LangKit to WhyLabs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production integrations should take a look at our [docs](https://docs.whylabs.ai/docs/usecases-streaming/) and use something like the code sample below\n",
    "\n",
    "```python\n",
    "from whylogs.experimental.core.metrics.udf_metric import generate_udf_schema\n",
    "from whylogs.core.schema import DeclarativeSchema\n",
    "import whylogs as why\n",
    "from langkit.sentiment import *\n",
    "from langkit.textstat import *\n",
    "from langkit.regexes import *\n",
    "from langkit.themes import *\n",
    "from langkit.toxicity import *\n",
    "\n",
    "langkit_schema = DeclarativeSchema(generate_udf_schema())\n",
    "\n",
    "# initialize a persistent whylogs logger as a telemetry agent, that will periodically\n",
    "# write the metrics to a configured location.\n",
    "telemetry_agent = why.logger(\n",
    "    schema=langkit_schema,\n",
    "    mode=\"rolling\",\n",
    "    interval=5,\n",
    "    when=\"M\",\n",
    ")\n",
    "\n",
    "# Adds a whylabs writer (it will write the aggregate statistics and metrics to WhyLabs)\n",
    "# you can also use 'local' for local file store, or s3, and various other writers.\n",
    "telemetry_agent.append_writer(\"whylabs\")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Generating telemetry on your prompts and responses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration, let's use some more archived response/prompts data from Hugging Face, or you can interact with GPT to see how it works in real time if you already have an OpenAI api key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 'log-0QApIE2LzK62kU22')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from whylogs.api.writer.whylabs import WhyLabsWriter\n",
    "import whylogs as why\n",
    "\n",
    "\n",
    "archived_chats = load_dataset('alespalla/chatbot_instruction_prompts', split=\"test\", streaming=True)\n",
    "archive_iterator = iter(archived_chats)\n",
    "prompt_response = next(archive_iterator)\n",
    "profile = why.log(prompt_response, schema=langkit_schema).profile()\n",
    "profile.track({\"prompt\":\"Wow this is amazing!! I can believe you were able to do that.\", \"response\": \"As an AI language model\"})\n",
    "\n",
    "writer = WhyLabsWriter()\n",
    "writer.write(profile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell read an archived prompt/reponse pair profiled it and sent these privacy preserving metrics to WhyLabs.\n",
    "\n",
    "\n",
    "> üìù **Note**: *you need to have defined a WhyLabs API Key, and other environment variables above in order to write profiles to WhyLabs.*\n",
    "\n",
    "Next lets profile a few batches of data and send their profiles to WhyLabs to simulate an LLM application integrated with LangKit that has been running for over a week. From this we can visualize how the profile's metrics change over time. From the WhyLabs platform you can setup monitors on your LLM application using these LangKit extracted metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "# For demo purposes only, for production use case see the above telemetry_agent example\n",
    "print(f\"Log prompt/responses for the past 7 days.\")\n",
    "batch_size = 10\n",
    "current_date = datetime.now(timezone.utc)\n",
    "for day in range(1, 7):\n",
    "  prompt_response = next(archive_iterator)\n",
    "  profile = why.log(prompt_response, schema=langkit_schema).profile()\n",
    "  archived_prompt_responses = []\n",
    "  dataset_date = current_date - timedelta(days=day)\n",
    "  print(f\"Downloading archived batch for {dataset_date}\")\n",
    "  for _ in range(10):\n",
    "    record = next(archive_iterator)\n",
    "    archived_prompt_responses.append(record)\n",
    "\n",
    "  for record in archived_prompt_responses:\n",
    "    profile.track(record)\n",
    "    print(\".\", end=\"\", flush=True)\n",
    "\n",
    "  profile.set_dataset_timestamp(dataset_date)\n",
    "  writer.write(profile)\n",
    "  print()\n",
    "print(\"Done. Go see your metrics on the WhyLabs dashboard!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Understanding LangKit telemetry in WhyLabs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, check your dashboard to verify everything went ok. At the __Profile__ tab, you should see something like this:\n",
    "\n",
    "![sentiment_in_whylabs.png](https://drive.google.com/uc?id=1YXDsg14eV8Kc7XctNK622FQjC4OFQW40)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on the libraries we use to gather some of these metrics see:\n",
    "* [textstat readability scores](https://pypi.org/project/textstat/)\n",
    "* [nltk sentiment](https://www.nltk.org/api/nltk.html)\n",
    "* [similarity scores](https://pypi.org/project/sentence-transformers/)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Monitor your LLM Application"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should give you a quick way to look at your extracted metrics on the prompts and responses, and these can be [monitored](https://docs.whylabs.ai/docs/monitor-manager#preset-monitors) over time! Try out the numerical drift preset monitor and preview results to see something like this:\n",
    "![monitor_preview.png](https://drive.google.com/uc?id=1z5FXumZAohX79f9ldV7LO6jMs7JNvghR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d39f874c9b8a97550ecbd783714b95e79c9b905449b34f44c40e3bf053b54b41"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
