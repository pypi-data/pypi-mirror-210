# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['datasieve', 'datasieve.transforms']

package_data = \
{'': ['*']}

install_requires = \
['pandas>=1.3.3,<2.0.0', 'scikit-learn>=1.2.0,<2.0.0']

setup_kwargs = {
    'name': 'datasieve',
    'version': '0.0.2',
    'description': 'This package implements a flexible data pipeline to help organize row removal (e.g. outlier removal) and feature modification (e.g. PCA)',
    'long_description': '# DataSieve\n\nDataSieve is very similar to the SKlearn Pipeline in that it:\n\n- fits an arbitrary series of transformations to an array X\n- transforms subsequent arrays of the same dimension according to the fit from the original X\n- inverse transforms arrays by inverting the series of transformations\n\nThis means that it follows the SKLearn API very closely, and in fact most of the methods inherit directly from SKLearn methods.\n\nThe main **difference** is that DataSieve allows for the manipulation of the y and sample_weight arrays in addition to the X array. This is useful if you find yourself wishing to use the SKLearn pipeline for:\n\n- removing outliers across your X, y, and sample_weights arrays according to simple or complex criteria\n- remove feature columns based on arbitrary criteria (e.g. low variance features)\n- change feature column names at certain transformations (e.g. PCA)\n- passing dynamic parameters to individual transforms of the pipeline\n- passing dataframes/arrays without worrying about converting to arrays and maintaining the proper feature columns\n\nThese improved flexibilities allow for more customized/creative transformations. For example, the included `DataSieveDBSCAN` has automated parameter fitting and outlier removal based on clustering. \n\nAn example would be someone who wants to use `SGDOneClassSVM` to detect and remove outliers from their data set before training:\n\n```python\nclass SVMOutlierExtractor(SGDOneClassSVM):\n    """\n    A subclass of the SKLearn SGDOneClassSVM that adds a transform() method\n    for removing detected outliers from X (as well as the associated y and\n    sample_weight if they are also furnished.\n    """\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def fit_transform(self, X, y=None, sample_weight=None, feature_list=None, **kwargs):\n        self.fit(X, y, sample_weight=sample_weight)\n        return self.transform(X, y, sample_weight=sample_weight)\n\n    def fit(self, X, y=None, sample_weight=None, feature_list=None, **kwargs):\n        super().fit(X, y=y, sample_weight=sample_weight)\n        return X, y, sample_weight, feature_list\n\n    def transform(self, X, y=None, sample_weight=None, feature_list=None, **kwargs):\n        y_pred = self.predict(X)\n\n        X, y, sample_weight = remove_outliers(X, y, sample_weight, y_pred)\n\n        num_tossed = len(y_pred) - len(X)\n        if num_tossed > 0:\n            logger.info(\n                f"SVM detected {num_tossed} data points"\n                "as outliers."\n            )\n\n        return X, y, sample_weight, feature_list\n\n    def inverse_transform(self, X, y=None, sample_weight=None, feature_list=None, **kwargs):\n        """\n        Unused, pass through X, y, sample_weight, and feature_list\n        """\n        return X, y, sample_weight, feature_list\n```\n\n\nAs shown here, the `fit()` method is actually identical to the SKLearn `fit()` method, but the `transform()` removes data points from X, y, and sample_weight for any outliers detected in the `X` array.\n\n\n# Usage\nThe user builds the pipeline similarly to SKLearn:\n\n```python\n    from datasieve.pipeline import Pipeline\n    from datasieve.transforms import DataSieveMinMaxScaler, DataSievePCA, DataSieveVarianceThreshold, SVMOutlierExtractor\n\n    feature_pipeline = Pipeline([\n        ("detect_constants", DataSieveVarianceThreshold(threshold=0)),\n        ("pre_svm_scaler", DataSieveMinMaxScaler(feature_range=(-1, 1)))\n        ("svm", SVMOutlierExtractor()),\n        ("pre_pca_scaler", DataSieveMinMaxScaler(feature_range=(-1, 1)))\n        ("pca", DataSievePCA(n_components=0.95),\n        ("post_pca_scaler", DataSieveMinMaxScaler(feature_range=(-1, 1)))\n    ])\n\n```\n\nOnce the pipeline is built, it can be fit and transformed similar to a SKLearn pipeline:\n\n```python\nX, y, sample_weight = feature_pipeline.fit_transform(X, y, sample_weight)\n```\n\nThis pipeline demonstrates the various components of `DataSieve` which are missing from SKLearn\'s pipeline. A dataframe `X` (if desired, else users can input a simply array without column names) is input with its associated `y` and `sample_weight` arrays/vectors. The `VarianceThreshold` will first detect and remove any features that have zero variance in X, the `SVMOutlierExtractor` will fit `SGDOneClassSVM` to `X` and then remove the detected outliers in `X`, while also propagating those row removals from `y` and `sample_weight`. Finally, the `PCA` will be fit to the remaining `X` array with the features count changing and getting renamed. The returned `X` dataframe will have the correctly named column features/count, and equal row counts across the `X`, `y`, and `sample_weight` arrays.\n\nNext, the `feature_pipeline` can then be used to transform other datasets with the same input feature dimension:\n\n```python\nX2, _, _ = feature_pipeline.transform(X2)\n\n```\n\nFinally, similar to SKLearn\'s pipeline, the `feature_pipeline` can be used to inverse_transform an array `X3` array that has the same dimensions as the returned `X` array from the pipeline:\n\n```python\nXinv, _ ,_ = feature_pipeline.inverse_transform(X)\n```\n\n## Data removal\n\nThe command `feature_pipeline.fit_transform(X, y, sample_weight)` fits each pipeline step to `X`, and transforms `X` according to each step\'s `transform()` method. In some cases, this will not affect `y` or `sample_weight`. For example, `FlowdaptMinMaxScaler` simply scales `X` and saves the normalization information.  Meanwhile, in the `SVMOutlierExtractor`, `.fit()` will fit an SVM to `X` and `.transform()` will remove any detected outliers from `X`. Typical `Scikit-Learn` pipelines do not remove those data points from `y` and `sample_weight`. Luckily, the `FlowdaptPipeline` takes care of the "associated removal" of the same outlier data points from `y` and `sample_weight`. \n\n## Feature modification\n\nAnother feature is demonstrated in the `FlowdaptPCA`, which fits a PCA transform to `X` and then transforms `X` to principal components. This dimensionality reduction means that the features are no longer the same, instead they are now `PC1`, `PC2` ... `PCX`. `FlowdaptPipeline` handles the feature renaming at that step (which is not a feature available in the `Scikit-Learn` pipeline). Similar to `FlowdaptPCA`, the `FlowdaptVarianceThreshold` subclasses the `Scikit-Learn` `VarianceThreshold` which is geared toward removing features that have a low variance. `FlowdaptVarianceThreshold` ensures that the removed features are properly handled when `X` passes through this part of the pipeline.\n\n## Adding a custom step\n\nEach step of the `Pipeline` *must* contain the following methods:\n\n```python\nclass MyTransformer:\n    def fit_transform(self, X, y=None, sample_weight=None, feature_list=None, **kwargs):\n        X, y, sample_weight = self.fit(X, y, sample_weight)\n        X, y, sample_weight = self.transform(X, y, sample_weight)\n        return X, y, sample_weight, feature_list\n\n    def fit(self, X, y=None, sample_weight=None, feature_list=None, **kwargs):\n        return X, y, sample_weight, feature_list\n\n    def transform(self, X, y=None, sample_weight=None, feature_list=None, **kwargs):\n        return X, y, sample_weight, feature_list\n\n    def inverse_transform(self, X, y=None, sample_weight=None, feature_list=None, **kwargs):\n        return X, y, sample_weight, feature_list\n```\n\nThis is because these four methods are called automatically by the `Pipeline` object. In most cases, the goal is to add functionality for an existing transformer from the `Scikit-Learn` library. Here is an example of subclassing the `MinMaxScaler` to work with the `FlowdaptPipeline`:\n\n```python\nclass DataSieveMinMaxScaler(MinMaxScaler):\n    """\n    A subclass of the SKLearn MinMaxScaler that ensures fit, transform, fit_transform and\n    inverse_transform all take the full set of params X, y, sample_weight (even if they\n    are unused) to follow the FlowdaptPipeline API.\n    """\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def fit_transform(self, X, y=None, sample_weight=None, feature_list=None, **kwargs):\n        super().fit(X)\n        X = super().transform(X)\n        return X, y, sample_weight, feature_list\n\n    def fit(self, X, y=None, sample_weight=None, feature_list=None, **kwargs):\n        super().fit(X)\n        return X, y, sample_weight, feature_list\n\n    def transform(self, X, y=None, sample_weight=None, feature_list=None, **kwargs):\n        X = super().transform(X)\n        return X, y, sample_weight, feature_list\n\n    def inverse_transform(self, X, y=None, sample_weight=None, feature_list=None, **kwargs):\n        return super().inverse_transform(X), y, sample_weight, feature_list\n```\n\n\n\n# Installation\n\nThe easiest way to install `datasieve` is with:\n\n```\npip install datasieve\n```\n\nbut you can also clone this repository and install it with:\n\n```\ngit clone https://github.com/emergentmethods/datasieve.git\ncd datasieve\npoetry install\n```\n\n\n# License\n\nCopyright (c) 2023 DataSieve\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the "Software"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.',
    'author': 'Robert Caulk',
    'author_email': 'None',
    'maintainer': 'None',
    'maintainer_email': 'None',
    'url': 'None',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'python_requires': '>=3.8,<4.0',
}


setup(**setup_kwargs)
